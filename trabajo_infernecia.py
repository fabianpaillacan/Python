# -*- coding: utf-8 -*-
"""trabajo_infernecia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qmj2r2j9cFk8M5Nb46ZESuhpLSr4apKh

**Importamos lo necesario, leemos el archivo .csv y ademas filtramos las filas no utiles debido a falta de informacion en ellas.**
"""

#Integrantes: Fabian Paillacan \ Carlos Gallegos \ Pablo Nasta
#Seccion: 411

#Base de datos utilizada https://www.kaggle.com/datasets/uciml/adult-census-income

#Para que compile correctamente, se debe descargar la base de datos de la pagina kaggle y subirla al colab, una vez subida, el programa funcionara correctamente
import tensorflow as tf
import csv
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statistics
from scipy import stats
from scipy.stats import chi2_contingency
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow import keras
from sklearn.metrics import confusion_matrix

datos = pd.read_csv('adult.csv') # Importar datos del CSV
# si la linea de arriba tira error asegurese que el archivo adult.csv se encuentra subido, ademas, tiene que llamarse literalmente adult.csv
tamano_muestra = datos.shape[0]
print("Tamaño de la muestra:", tamano_muestra) # Imprimir la muestra sin filtrar

datos = datos.replace('?', np.nan)
datos = datos.dropna() # Elimina filas con valores faltantes


datos['income']=datos['income'].map({'<=50K': 0, '>50K': 1, '<=50K.': 0, '>50K.': 1}) # La red neuronal predicira los salarios, por eso es necesario transformar la columna 'income' a 0 y 1 dependiendo si es mayor a 50k o no

"""**Observaremos el tamaño de la muestra total pero ahora filtrada, recordar que se excluyeron las filas que no utilizaremos**



"""

tamano_muestra2 = datos.shape[0]
print("Tamaño de la muestra:", tamano_muestra2) # Imprimir el tamaño de la muestra filtrada

print("Podemos decir que se filtraron:",tamano_muestra - tamano_muestra2,"filas de la base de datos" ) # Imprimir la cantidad de datos filtrados

"""**Implementacion de codigo para graficar la cantidad de personas que hay por edad en un grafico de barras**"""

edad_contador = datos['age'].value_counts()

edad_contador = edad_contador.sort_index() # Ordenar las edades de manera ascendente

plt.bar(edad_contador.index, edad_contador.values) # Definimos una tabla

plt.title('Distribución de Edades')
plt.xlabel('Edad')
plt.ylabel('Cantidad de Personas')

plt.show()

"""**Implementacion de codigo para graficar la cantidad de Hombres y Mujeres que hay en la muestra**"""

conteo_genero = datos['sex'].value_counts()

# Creacion de un gráfico de barras
plt.bar(conteo_genero.index, conteo_genero.values)
plt.xticks(conteo_genero.index, ['Hombres', 'Mujeres'])
# Personalizacion del gráfico
plt.title('Distribución de Género')
plt.xlabel('Género')
plt.ylabel('Cantidad de Personas')

# Imprimir el gráfico
plt.show()

tamanio_muestra_hombres = datos['sex'].value_counts()['Male']
tamanio_muestra_mujeres = datos['sex'].value_counts()['Female']

print("Cantidad de hombres:", tamanio_muestra_hombres)
print("Cantidad de mujeres:", tamanio_muestra_mujeres)

"""**Grafico para saber la cantidad de personas que ganan mas de 50k vs las que ganan menos de 50k**"""

conteo_income = datos['income'].value_counts()

plt.bar(conteo_income.index, conteo_income.values) # Crear el gráfico de barras

plt.xticks(conteo_income.index, ['<=50K', '>50K'])

plt.title('Distribución de Ingresos')
plt.xlabel('Ingreso')
plt.ylabel('Cantidad de Personas')

plt.show()

"""**Grafico distribucion de ingresos de hombres**"""

datos_hombres = datos[datos['sex'] == 'Male']

conteo_income = datos_hombres['income'].value_counts()

plt.bar(conteo_income.index, conteo_income.values) # Crear el gráfico de barras

plt.xticks(conteo_income.index, ['<=50K', '>50K'])

plt.title('Distribución de Ingresos de Hombres')
plt.xlabel('Ingreso')
plt.ylabel('Cantidad de Hombres')

plt.show()

"""**Grafico distribucion de ingresos de mujeres**"""

datos_mujeres = datos[datos['sex'] == 'Female']

conteo_income = datos_mujeres['income'].value_counts()

plt.bar(conteo_income.index, conteo_income.values) # Crear el gráfico de barras

plt.xticks(conteo_income.index, ['<=50K', '>50K'])

plt.title('Distribución de Ingresos en Mujeres')
plt.xlabel('Ingreso')
plt.ylabel('Cantidad de Mujeres')

plt.show()

"""**Apreciaremos los datos que tenemos en nuetra base de datos**"""

datos

"""**Quitamos las columnas que no usaremos para implementar la red neuronal, en este caso "education" y "fnlwgt".
El resto seran mis variables de entradas, osea, variables predictoras que me ayudaran a predecir el income. Las definiremos como categorical.**
"""

categorical = ['age', 'workclass',   'education.num',
       'marital.status', 'occupation', 'relationship', 'race', 'sex',
       'capital.gain', 'capital.loss', 'hours.per.week', 'native.country']

# Seleccionar características de entrada y salida
X = datos[['age', 'workclass', 'education.num',
       'marital.status', 'occupation', 'relationship', 'race', 'sex',
       'capital.gain', 'capital.loss', 'hours.per.week', 'native.country']]
y = datos['income']


le = LabelEncoder() # Transformar género a valores binarios

for indice in categorical:
  X[indice] = le.fit_transform(X[indice]) #Con esta funcion transformamos basicamente las palabras que aparecen en la tabla a numeros. Es decir, de cualitativo a cuantitativo
  y = le.fit_transform(y)

"""**Apreciaremos que todas las filas que tenian variables cualitativas se reemplazaron por numeros, en un termino mas de programacion, Las variables tipo char pasaron a ser tipo int para que funcione la red neuronal**"""

X

"""**Estadisticas Descriptivas**"""

datos_hombres = datos[datos['sex'] == 'Male']
datos_mujeres = datos[datos['sex'] == 'Female']


print("-----------------------Estadisticas generales-----------------------")

print("Media de la edad:", np.mean(datos['age'])) #funcion de python que saca la media
print("Varianza de la edad:", np.std(datos['age'])) #funcion de python que saca la varianza de las edades
print("Rango de la edad:", max(datos['age'])-min(datos['age'])) #funcion de python que saca el rango de edades
print("Moda de la edad:", statistics.mode(datos['age'])) #funcion de python que saca la moda de edades de la base de datos

print("-----------------------Estadisticas en hombres-----------------------")

#Estadistica en hombres
print("Media de la edad en hombres:", np.mean(datos_hombres['age']))
print("Varianza de la edad en hombres:", np.std(datos_hombres['age']))
print("Rango de la edad en hombres:", max(datos_hombres['age'])-min(datos_hombres['age']))
print("Moda de la edad en hombres:", statistics.mode(datos_hombres['age']))

print("-----------------------Estadisticas en mujeres-----------------------")
#Estadistica en mujeres
print("Media de la edad en mujeres:", np.mean(datos_mujeres['age']))
print("Varianza de la edad en mujeres:", np.std(datos_mujeres['age']))
print("Rango de la edad en mujeres:", max(datos_mujeres['age'])-min(datos_mujeres['age']))
print("Moda de la edad en mujeres:", statistics.mode(datos_mujeres['age']))


# Dividir datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #dividimos los datos, el test size nos indica mi conjunto de prueba, osea, el 20%, por lo tanto podemos decir que se entrenara un 80% de los datos

# Escalar características de entrada
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  #funcion que estandariza las características de los conjuntos de datos de entrenamiento y prueba.
X_test_scaled = scaler.transform(X_test)

y_test.shape

"""**Creamos y entrenamos nuestro modelo de red neuronal.**"""

# Creacion modelo de red neuronal
model = keras.Sequential([
    keras.layers.Dense(units=124, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(units=1) #capa de salida que es solo 1
]) # Definimos una cantidad de neuronas en units para que nuestro modelo pueda aprender bien, y habra una salida (el income  o salario).

# Compilacion del modelo
model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='binary_crossentropy', metrics=['accuracy']) #compilamos nuestro modelo, con una tasa de peso de 0.001 para que el modelo de aprendizaje sea mas bueno pero no tan lento ni tan rapido

# Entrenamiento del modelo
history = model.fit(X_train_scaled, y_train, epochs=30, verbose=1) #aca es donde se entrena, el epochs es la cantidad de veces que se entrenara, se puede cambiar el numero pero hay un momento en que nuestra red neuronal deja de aprender

# Evaluacion del modelo en datos de prueba
loss = model.evaluate(X_test_scaled, y_test, verbose=2)

print("**Modelo neuronal entrenado exitosamente**")

y_pred_prob = model.predict(X_test_scaled)
y_pred = np.where(y_pred_prob > 0.5, 1, 0) # cantidad de datos que quedaron fuera del entrenamiento, es decir, contra los que se van a probar

from sklearn.metrics import classification_report

report = classification_report(y_test, y_pred)
print(report)

"""**Aqui veremos la prueba de si se entreno bien nuestra red neuronal, es bastante parecido al tema del error tipo 1 y 2**"""

from sklearn import metrics
confusion_matrix = metrics.confusion_matrix(y_test, y_pred)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])
cm_display.plot()
plt.show()